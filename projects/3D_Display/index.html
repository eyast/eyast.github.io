<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mind Blowing 3D display at home | Eyas Taifour</title> <meta name="author" content="Eyas Taifour"> <meta name="description" content="Transforming my Living Room TV into a Mind-blowing 3D display"> <meta name="keywords" content="data-science, neural-networks, pytorch, keras, microsoft"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://eyast.github.io/projects/3D_Display/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Eyas </span>Taifour</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Portfolio</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mind Blowing 3D display at home</h1> <p class="post-description">Transforming my Living Room TV into a Mind-blowing 3D display</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/puzzle_final/final-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/puzzle_final/final-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/puzzle_final/final-1400.webp"></source> <img src="/assets/img/puzzle_final/final.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Custom-made, forced perspective renderer" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A custom made software analyzes my location, and renders a 3D scene epxplicitly for my viewing angle. </div> <p>PS: Co-authored by ChatGPT (hence, very wordy).</p> <h1 id="project-description">Project Description</h1> <p>Amidst the pandemic, many of us adjusted to working from home. In search of a captivating home entertainment experience, I transformed my living room TV into a mesmerizing 3D display. Inspired by the desire for immersion, I started exploring the world of real-time perspective correction. I built an app that tracks me in my living room, and accordingly renders a 3D scene on my TV, for my own viewing, with the correct perspective that matches my position to create an optical illusion. Basically, an exercise of building a real-time <a href="https://en.wikipedia.org/wiki/Forced_perspective" rel="external nofollow noopener" target="_blank">forced perspective</a> mechanism.</p> <p>The app uses computer vision and a webcam to detect humans, isolate their figures, and extract body positions in a 2D space. This data is then transformed into a three-dimensional representation of the user’s position in the room. By mapping the 2D body position to a virtual 3D space, the app provides an immersive experience by accurately locating the user’s head within the living room. It combines advanced technology to push the boundaries between the real and virtual worlds, enhancing home entertainment experiences.</p> <p>Once the 2D body position is translated into a 3D representation, a game rendering engine (Unity) takes the 3D points representing the location of the user’s head in the living room and utilizes them to recreate an immersive 3D environment. It meticulously constructs a virtual space that mirrors your physical surroundings, accounting for depth, perspective, and spatial relationships. With careful attention to detail, the software renders this virtual environment, transforming it into a captivating visual display that can be seamlessly projected onto your living room TV. As a result, the user is transported into a mesmerizing realm where the boundaries between reality and the virtual world dissolve, providing an unparalleled and truly immersive home entertainment experience.</p> <p>From a high level perspective, the application is composed of two executables:</p> <ol> <li>A docker container running on a <a href="https://www.nvidia.com/en-au/autonomous-machines/embedded-systems/jetson-nano/" rel="external nofollow noopener" target="_blank">Nvidia Jetson Nano</a> that infers the viewer’s position in my living room, and serves that to a downstream client as a JSON reply.</li> <li>The client (Unity in this case), queries the API once per Frame Refresh, and uses this information to modify the Scene’s. A 3D object is placed behind the TV, and is rendered.</li> </ol> <p>THe diagram below shows the different components at work, at every single Frame refresh. The Jetson Nano includes the following sub-components:</p> <ol> <li>A webcam - a <a href="https://www.logitech.com/en-au/products/webcams/c920-pro-hd-webcam.960-000770.html" rel="external nofollow noopener" target="_blank">Logicom c920 webcam</a>.</li> <li>PoseNet - <a href="https://github.com/dusty-nv/jetson-inference/blob/master/docs/posenet.md" rel="external nofollow noopener" target="_blank">a built-in Neural Network</a> that takes in an image, and returns the location of BodyPose KeyPoints.</li> <li>A custom Features Creator moduled (detailed below). The FeaturesCreator transforms the raw data returned by PoseNet to features I use in my application.</li> <li>A custom Neural Network, that transforms 2D pixel values of the previous steps, into 3D vector space mapped to my living room.</li> <li> <a href="https://en.wikipedia.org/wiki/Kalman_filter" rel="external nofollow noopener" target="_blank">A Kalman filter</a>, which I use to reduce the noise returned from my Neural Network.</li> <li> <a href="https://fastapi.tiangolo.com/" rel="external nofollow noopener" target="_blank">FastAPI</a>, which provides a JSON answer to anyone who access / on HTTP.</li> <li> <a href="https://unity.com/pages/unity-pro-buy-now?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=cc_dd_upr_sapac_sapac-t1_en_pu_sem-gg_acq_br-pr_2023-01_brand-st1_cc3022_ev-br_id:71700000106719832&amp;utm_content=cc_dd_upr_apac_pu_sem_gg_ev-br_pros_x_npd_cpc_kw_sd_all_x_x_brand_id:58700008276350171&amp;utm_term=unity&amp;&amp;&amp;&amp;&amp;gad=1&amp;gclid=Cj0KCQjwj_ajBhCqARIsAA37s0wD9evaH3lHrzZ4UXZYJU8WgJ9Jt39LSVC7Vnx_l_goCvR6FmMKfTUaArlyEALw_wcB&amp;gclsrc=aw.ds" rel="external nofollow noopener" target="_blank">A Unity</a> client, in which the Camera’s Position is set to what the FastAPI endpoint returned.</li> </ol> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1686284940762" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:1523.859375px;" viewbox="-50 -10 1523.859375 746"><style>#mermaid-1686284940762 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1686284940762 .node circle,#mermaid-1686284940762 .node ellipse,#mermaid-1686284940762 .node polygon,#mermaid-1686284940762 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1686284940762 .node.clickable{cursor:pointer}#mermaid-1686284940762 .arrowheadPath{fill:#333}#mermaid-1686284940762 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1686284940762 .edgeLabel{background-color:#e8e8e8}#mermaid-1686284940762 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1686284940762 .cluster text{fill:#333}#mermaid-1686284940762 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1686284940762 .actor{stroke:#ccf;fill:#ececff}#mermaid-1686284940762 text.actor{fill:#000;stroke:none}#mermaid-1686284940762 .actor-line{stroke:grey}#mermaid-1686284940762 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1686284940762 .messageLine0,#mermaid-1686284940762 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1686284940762 #arrowhead{fill:#333}#mermaid-1686284940762 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1686284940762 .messageText{fill:#333;stroke:none}#mermaid-1686284940762 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1686284940762 .labelText,#mermaid-1686284940762 .loopText{fill:#000;stroke:none}#mermaid-1686284940762 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1686284940762 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1686284940762 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1686284940762 .section{stroke:none;opacity:.2}#mermaid-1686284940762 .section0{fill:rgba(102,102,255,.49)}#mermaid-1686284940762 .section2{fill:#fff400}#mermaid-1686284940762 .section1,#mermaid-1686284940762 .section3{fill:#fff;opacity:.2}#mermaid-1686284940762 .sectionTitle0,#mermaid-1686284940762 .sectionTitle1,#mermaid-1686284940762 .sectionTitle2,#mermaid-1686284940762 .sectionTitle3{fill:#333}#mermaid-1686284940762 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1686284940762 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1686284940762 .grid path{stroke-width:0}#mermaid-1686284940762 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1686284940762 .task{stroke-width:2}#mermaid-1686284940762 .taskText{text-anchor:middle;font-size:11px}#mermaid-1686284940762 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1686284940762 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1686284940762 .taskText0,#mermaid-1686284940762 .taskText1,#mermaid-1686284940762 .taskText2,#mermaid-1686284940762 .taskText3{fill:#fff}#mermaid-1686284940762 .task0,#mermaid-1686284940762 .task1,#mermaid-1686284940762 .task2,#mermaid-1686284940762 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1686284940762 .taskTextOutside0,#mermaid-1686284940762 .taskTextOutside1,#mermaid-1686284940762 .taskTextOutside2,#mermaid-1686284940762 .taskTextOutside3{fill:#000}#mermaid-1686284940762 .active0,#mermaid-1686284940762 .active1,#mermaid-1686284940762 .active2,#mermaid-1686284940762 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1686284940762 .activeText0,#mermaid-1686284940762 .activeText1,#mermaid-1686284940762 .activeText2,#mermaid-1686284940762 .activeText3{fill:#000!important}#mermaid-1686284940762 .done0,#mermaid-1686284940762 .done1,#mermaid-1686284940762 .done2,#mermaid-1686284940762 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1686284940762 .doneText0,#mermaid-1686284940762 .doneText1,#mermaid-1686284940762 .doneText2,#mermaid-1686284940762 .doneText3{fill:#000!important}#mermaid-1686284940762 .crit0,#mermaid-1686284940762 .crit1,#mermaid-1686284940762 .crit2,#mermaid-1686284940762 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1686284940762 .activeCrit0,#mermaid-1686284940762 .activeCrit1,#mermaid-1686284940762 .activeCrit2,#mermaid-1686284940762 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1686284940762 .doneCrit0,#mermaid-1686284940762 .doneCrit1,#mermaid-1686284940762 .doneCrit2,#mermaid-1686284940762 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1686284940762 .activeCritText0,#mermaid-1686284940762 .activeCritText1,#mermaid-1686284940762 .activeCritText2,#mermaid-1686284940762 .activeCritText3,#mermaid-1686284940762 .doneCritText0,#mermaid-1686284940762 .doneCritText1,#mermaid-1686284940762 .doneCritText2,#mermaid-1686284940762 .doneCritText3{fill:#000!important}#mermaid-1686284940762 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1686284940762 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1686284940762 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1686284940762 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1686284940762 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1686284940762 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1686284940762 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1686284940762 #compositionEnd,#mermaid-1686284940762 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284940762 #aggregationEnd,#mermaid-1686284940762 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1686284940762 #dependencyEnd,#mermaid-1686284940762 #dependencyStart,#mermaid-1686284940762 #extensionEnd,#mermaid-1686284940762 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284940762 .branch-label,#mermaid-1686284940762 .commit-id,#mermaid-1686284940762 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style> <style>#mermaid-1686284940762{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style> <g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">Webcam</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">PoseNet</tspan></text></g><g><line id="actor2" x1="475" y1="5" x2="475" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="400" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">FeatureCreator</tspan></text></g><g><line id="actor3" x1="675" y1="5" x2="675" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="600" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">CustomNet</tspan></text></g><g><line id="actor4" x1="875" y1="5" x2="875" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="800" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">KalmanFilter</tspan></text></g><g><line id="actor5" x1="1075" y1="5" x2="1075" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="1000" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1075" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1075" dy="0">FastAPI</tspan></text></g><g><line id="actor6" x1="1275" y1="5" x2="1275" y2="735" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="1200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1275" dy="0">Unity</tspan></text></g><defs><marker id="arrowhead" refx="5" refy="2" markerwidth="6" markerheight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerwidth="15" markerheight="8" orient="auto" refx="16" refy="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="1175" y="118" class="messageText" style="text-anchor: middle;">If there's a human in the Frame, what's the position of their head in 3D space?</text><line x1="1275" y1="125" x2="1075" y2="125" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="575" y="153" class="messageText" style="text-anchor: middle;">Take an image</text><line x1="1075" y1="160" x2="75" y2="160" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="70" y="162" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="577.5" y="188" class="messageText" style="text-anchor: middle;">Image returned.</text><line x1="80" y1="195" x2="1075" y2="195" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="675" y="223" class="messageText" style="text-anchor: middle;">Where are the Body Pose KeyPoints?</text><line x1="1075" y1="230" x2="275" y2="230" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="270" y="232" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="677.5" y="258" class="messageText" style="text-anchor: middle;">Raw_data returned.</text><line x1="280" y1="265" x2="1075" y2="265" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="775" y="293" class="messageText" style="text-anchor: middle;">What are the features in this raw data point?</text><line x1="1075" y1="300" x2="475" y2="300" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="470" y="302" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="777.5" y="328" class="messageText" style="text-anchor: middle;">Features returned.</text><line x1="480" y1="335" x2="1075" y2="335" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="875" y="363" class="messageText" style="text-anchor: middle;">Here's a list of features - Infer the 3D location of the camera/head in the room</text><line x1="1075" y1="370" x2="675" y2="370" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="670" y="372" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="877.5" y="398" class="messageText" style="text-anchor: middle;">Location in 3D space returned.</text><line x1="680" y1="405" x2="1075" y2="405" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="975" y="433" class="messageText" style="text-anchor: middle;">Filter this noisy data</text><line x1="1075" y1="440" x2="875" y2="440" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="870" y="442" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="977.5" y="468" class="messageText" style="text-anchor: middle;">Filtered location returned.</text><line x1="880" y1="475" x2="1075" y2="475" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="1175" y="503" class="messageText" style="text-anchor: middle;">3D location of camera returned in JSON.</text><line x1="1075" y1="510" x2="1275" y2="510" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="1275" y="538" class="messageText" style="text-anchor: middle;">Modify the scene according to 3D location.</text><path d="M 1275,545 C 1335,535 1335,575 1275,565" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></path></g><g><text x="1275" y="603" class="messageText" style="text-anchor: middle;">Render the scene.</text><path d="M 1275,610 C 1335,600 1335,640 1275,630" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></path></g><g><line x1="60" y1="75" x2="1423.859375" y2="75" class="loopLine"></line><line x1="1423.859375" y1="75" x2="1423.859375" y2="650" class="loopLine"></line><line x1="60" y1="650" x2="1423.859375" y2="650" class="loopLine"></line><line x1="60" y1="75" x2="60" y2="650" class="loopLine"></line><polygon points="60,75 110,75 110,88 101.6,95 60,95" class="labelBox"></polygon><text x="67.5" y="90" fill="black" class="labelText"><tspan x="67.5" fill="black">loop</tspan></text><text x="741.9296875" y="90" fill="black" class="loopText" style="text-anchor: middle;"><tspan x="741.9296875" fill="black">[ everyFrame ]</tspan></text></g><g><rect x="0" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">Webcam</tspan></text></g><g><rect x="200" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">PoseNet</tspan></text></g><g><rect x="400" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">FeatureCreator</tspan></text></g><g><rect x="600" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">CustomNet</tspan></text></g><g><rect x="800" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">KalmanFilter</tspan></text></g><g><rect x="1000" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1075" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1075" dy="0">FastAPI</tspan></text></g><g><rect x="1200" y="670" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1275" y="702.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1275" dy="0">Unity</tspan></text></g></svg> </div> <h2 id="components">Components</h2> <p>To reduce any downstream error accumulation, I needed first to ensure the camera stays at a fixed point in the room. It was critical that nothing static changes position, therefore the first thing I did was creating a chassis that holds the camera firmly at a specific static point, in my case on top of the TV.</p> <p>The camera was originally planned to be connected to a Raspberry Pi, but after further consideration and to enable some room to experiment different technologies, the app was built in a Docker container on a NVidia Jetson Nano serving inference data through FastAPI as a REST API. This setup decouples the application components clearly, delineating a responsibility to determine the person’s location from rendering a scene. I was hoping to experiment with other technologies, thanks to this decoupling (for example, try to add a new network instead of the built-in, or use another rendering engine such as unreal, etc..). Unity, a popular game development platform, leverages the REST API to access the inference results, allowing for the integration of the captured 3D body position data into the virtual environment.</p> <p>The decision to opt for the Nvidia Jetson Nano was driven by the desire to leverage its onboard processing capabilities for running neural networks. This not only streamlines the overall setup but also enhances the system’s efficiency by offloading the processing workload from external devices. Additionally, the Nano’s versatility allows it to serve a dual purpose by utilizing the same device for rendering the scene on the TV. This integration further enhances the synchronization and fluidity of the overall experience, providing a seamless transition between the captured 3D environment and its projection on the television screen.</p> <h3 id="3d-parts">3D parts</h3> <p>The Logicom c920 chassis was designed in Autodesk Fusion, in an iterative manner, and printed on a Creality S3 Pro V1. With this chassis, the camera was stable and forced to remain in place.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/fusion_cam-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/fusion_cam-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/fusion_cam-1400.webp"></source> <img src="/assets/img/3ddisplay/fusion_cam.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Model I created in Fusion to hold the camera steadily on top of the TV." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/cam_printed-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/cam_printed-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/cam_printed-1400.webp"></source> <img src="/assets/img/3ddisplay/cam_printed.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Printed on Creality S3 Pro v1." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Chassis for the Logicom C920 webcam. Fusion render and 3D print displayed. </div> <h3 id="powering-the-nano-with-an-adequate-power-supply">Powering the Nano with an adequate power supply</h3> <p>A note on getting the correct power supply and storage: The Nvidia Jetson Nano relies on a 5V 2A power supply to ensure it receives sufficient power to drive its GPU effectively. Without a suitable power supply, the Nano will decrease the power supply to the GPU, leading to a suboptimal user experience characterized by choppy performance. It is crucial to provide the Nano with the recommended power specifications to enable smooth and uninterrupted GPU functionality. The <a href="https://forums.developer.nvidia.com/t/power-supply-considerations-for-jetson-nano-developer-kit/71637" rel="external nofollow noopener" target="_blank">NVidia forums</a> provide a list of recommended power supplies, as well as instructions to configure the Nano to use the external power supply instead of the 5V USB-C path. In addition to the power supply requirements, it is important to note that the storage SD card for the Nvidia Jetson Nano should not only have a large capacity, preferably above 32GB, but also be fast. The choice of a fast SD card is crucial as it directly impacts the inference time of the system. A high-speed SD card enables quicker read and write operations, facilitating faster data access and processing, ultimately reducing the time it takes for the Nano to perform inference tasks. Therefore, selecting a storage SD card that combines ample capacity with fast transfer speeds is essential for optimizing the overall performance of the Jetson Nano.</p> <h4 id="nvidia-hello-world-repository">NVidia Hello World repository</h4> <p>I built my work on the excellent Hello World Nvidia <a href="https://github.com/dusty-nv/jetson-inference" rel="external nofollow noopener" target="_blank">repository</a>. The very first thing I did was a smoke test: I ran one of the built-in networks to validate that it works,. I cloned the “hello world” repository, and ran <code class="language-plaintext highlighter-rouge">python/examples/posenet.py</code> with the default <code class="language-plaintext highlighter-rouge">resnet18-body</code> backbone network, <code class="language-plaintext highlighter-rouge">keypoints</code> overlay and <code class="language-plaintext highlighter-rouge">0.15</code> acceptance threshold, capturing from <code class="language-plaintext highlighter-rouge">/dev/video0</code>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/PoseNet-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/PoseNet-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/PoseNet-1400.webp"></source> <img src="/assets/img/3ddisplay/PoseNet.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="PoseNet" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Running out-of-the-box PoseNet to see what the Nvidia Jetson Nano can do. Pretty cool. </div> <h4 id="setting-up-ssh-for-vscode-remote--github-auth">Setting up SSH for vscode remote + github auth</h4> <p>I configured <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh" rel="external nofollow noopener" target="_blank">SSH to work with Github on the Nano</a>.</p> <h4 id="creating-a-new-dockerfile-to-install-fastapi-instead-of-flask">Creating a new DockerFile to install FastAPI instead of Flask</h4> <p>Once the development environment is ready and a test network was ran, I looked at modifying the Dockerfil incorporating essential modifications and installing FastAPI and some other dependencies instead of Flask. While I initially aimed to leverage VSCode DevContainers for seamless development, I encountered limitations due to the unsupported Python version within the image. However, this obstacle became an opportunity for growth as I delved into the intricacies of debugging, mastering the usage of pdb.set_trace() and remote debugging. This exercise not only enhanced my troubleshooting skills but also equipped me with invaluable insights into optimizing and fine-tuning the software environment for optimal performance.</p> <h4 id="calibrating-the-camera">Calibrating the camera</h4> <p>WIP</p> <h4 id="collecting-training-data">Collecting training data</h4> <p>With the Docker container up and running, the next step in my journey was to build the application that will serve as the foundation for collecting training data. THe diagram below shows the high level architecture used to collect training data. It’s very similar to the previous diagram with few modificatoins:</p> <ul> <li>The custom neural network is not a component (yet).</li> <li>There is no Kalman Filtering, since we are not infering.</li> <li>The data is saved to a local CSV File.</li> </ul> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1686284941407" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:1476.9765625px;" viewbox="-50 -10 1476.9765625 576"><style>#mermaid-1686284941407 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1686284941407 .node circle,#mermaid-1686284941407 .node ellipse,#mermaid-1686284941407 .node polygon,#mermaid-1686284941407 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1686284941407 .node.clickable{cursor:pointer}#mermaid-1686284941407 .arrowheadPath{fill:#333}#mermaid-1686284941407 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1686284941407 .edgeLabel{background-color:#e8e8e8}#mermaid-1686284941407 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1686284941407 .cluster text{fill:#333}#mermaid-1686284941407 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1686284941407 .actor{stroke:#ccf;fill:#ececff}#mermaid-1686284941407 text.actor{fill:#000;stroke:none}#mermaid-1686284941407 .actor-line{stroke:grey}#mermaid-1686284941407 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1686284941407 .messageLine0,#mermaid-1686284941407 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1686284941407 #arrowhead{fill:#333}#mermaid-1686284941407 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1686284941407 .messageText{fill:#333;stroke:none}#mermaid-1686284941407 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1686284941407 .labelText,#mermaid-1686284941407 .loopText{fill:#000;stroke:none}#mermaid-1686284941407 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1686284941407 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1686284941407 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1686284941407 .section{stroke:none;opacity:.2}#mermaid-1686284941407 .section0{fill:rgba(102,102,255,.49)}#mermaid-1686284941407 .section2{fill:#fff400}#mermaid-1686284941407 .section1,#mermaid-1686284941407 .section3{fill:#fff;opacity:.2}#mermaid-1686284941407 .sectionTitle0,#mermaid-1686284941407 .sectionTitle1,#mermaid-1686284941407 .sectionTitle2,#mermaid-1686284941407 .sectionTitle3{fill:#333}#mermaid-1686284941407 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1686284941407 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1686284941407 .grid path{stroke-width:0}#mermaid-1686284941407 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1686284941407 .task{stroke-width:2}#mermaid-1686284941407 .taskText{text-anchor:middle;font-size:11px}#mermaid-1686284941407 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1686284941407 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1686284941407 .taskText0,#mermaid-1686284941407 .taskText1,#mermaid-1686284941407 .taskText2,#mermaid-1686284941407 .taskText3{fill:#fff}#mermaid-1686284941407 .task0,#mermaid-1686284941407 .task1,#mermaid-1686284941407 .task2,#mermaid-1686284941407 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1686284941407 .taskTextOutside0,#mermaid-1686284941407 .taskTextOutside1,#mermaid-1686284941407 .taskTextOutside2,#mermaid-1686284941407 .taskTextOutside3{fill:#000}#mermaid-1686284941407 .active0,#mermaid-1686284941407 .active1,#mermaid-1686284941407 .active2,#mermaid-1686284941407 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1686284941407 .activeText0,#mermaid-1686284941407 .activeText1,#mermaid-1686284941407 .activeText2,#mermaid-1686284941407 .activeText3{fill:#000!important}#mermaid-1686284941407 .done0,#mermaid-1686284941407 .done1,#mermaid-1686284941407 .done2,#mermaid-1686284941407 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1686284941407 .doneText0,#mermaid-1686284941407 .doneText1,#mermaid-1686284941407 .doneText2,#mermaid-1686284941407 .doneText3{fill:#000!important}#mermaid-1686284941407 .crit0,#mermaid-1686284941407 .crit1,#mermaid-1686284941407 .crit2,#mermaid-1686284941407 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1686284941407 .activeCrit0,#mermaid-1686284941407 .activeCrit1,#mermaid-1686284941407 .activeCrit2,#mermaid-1686284941407 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1686284941407 .doneCrit0,#mermaid-1686284941407 .doneCrit1,#mermaid-1686284941407 .doneCrit2,#mermaid-1686284941407 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1686284941407 .activeCritText0,#mermaid-1686284941407 .activeCritText1,#mermaid-1686284941407 .activeCritText2,#mermaid-1686284941407 .activeCritText3,#mermaid-1686284941407 .doneCritText0,#mermaid-1686284941407 .doneCritText1,#mermaid-1686284941407 .doneCritText2,#mermaid-1686284941407 .doneCritText3{fill:#000!important}#mermaid-1686284941407 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1686284941407 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1686284941407 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1686284941407 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1686284941407 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1686284941407 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1686284941407 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1686284941407 #compositionEnd,#mermaid-1686284941407 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284941407 #aggregationEnd,#mermaid-1686284941407 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1686284941407 #dependencyEnd,#mermaid-1686284941407 #dependencyStart,#mermaid-1686284941407 #extensionEnd,#mermaid-1686284941407 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284941407 .branch-label,#mermaid-1686284941407 .commit-id,#mermaid-1686284941407 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style> <style>#mermaid-1686284941407{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style> <g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">CSVFile</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Webcam</tspan></text></g><g><line id="actor2" x1="475" y1="5" x2="475" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="400" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">PoseNet</tspan></text></g><g><line id="actor3" x1="675" y1="5" x2="675" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="600" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">FeatureCreator</tspan></text></g><g><line id="actor4" x1="875" y1="5" x2="875" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="800" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">FastAPI</tspan></text></g><g><line id="actor5" x1="1075" y1="5" x2="1075" y2="565" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="1000" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1075" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1075" dy="0">HumanBeing</tspan></text></g><defs><marker id="arrowhead" refx="5" refy="2" markerwidth="6" markerheight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerwidth="15" markerheight="8" orient="auto" refx="16" refy="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="975" y="93" class="messageText" style="text-anchor: middle;">(GET /training) - Hi, I'd like to get the training web page please</text><line x1="1075" y1="100" x2="875" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="870" y="102" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="977.5" y="128" class="messageText" style="text-anchor: middle;">Sure, here it is</text><line x1="880" y1="135" x2="1075" y2="135" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="1075" y="163" class="messageText" style="text-anchor: middle;">Find where you physically are, in the room, in meters, away from a pre-determine origin point</text><path d="M 1075,170 C 1135,160 1135,200 1075,190" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></path></g><g><text x="975" y="228" class="messageText" style="text-anchor: middle;">(POST /training) - Here's my physical location in meters, aware from a pre-determined origin point</text><line x1="1075" y1="235" x2="875" y2="235" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="575" y="263" class="messageText" style="text-anchor: middle;">Take an image</text><line x1="875" y1="270" x2="275" y2="270" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="270" y="272" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="577.5" y="298" class="messageText" style="text-anchor: middle;">Image returned.</text><line x1="280" y1="305" x2="875" y2="305" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="675" y="333" class="messageText" style="text-anchor: middle;">Where are the Body Pose KeyPoints?</text><line x1="875" y1="340" x2="475" y2="340" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="470" y="342" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="677.5" y="368" class="messageText" style="text-anchor: middle;">Raw_data returned.</text><line x1="480" y1="375" x2="875" y2="375" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="775" y="403" class="messageText" style="text-anchor: middle;">What are the features in this raw data point?</text><line x1="875" y1="410" x2="675" y2="410" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="670" y="412" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="777.5" y="438" class="messageText" style="text-anchor: middle;">Features returned.</text><line x1="680" y1="445" x2="875" y2="445" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="475" y="473" class="messageText" style="text-anchor: middle;">Append an existing CSV file with the features (out of FeatureCreator) and labels (entered by the user)</text><line x1="875" y1="480" x2="75" y2="480" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="0" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">CSVFile</tspan></text></g><g><rect x="200" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Webcam</tspan></text></g><g><rect x="400" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">PoseNet</tspan></text></g><g><rect x="600" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">FeatureCreator</tspan></text></g><g><rect x="800" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">FastAPI</tspan></text></g><g><rect x="1000" y="500" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="1075" y="532.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="1075" dy="0">HumanBeing</tspan></text></g></svg> </div> <p>This data will play a pivotal role in constructing a neural network that takes a set of detected Body Pose KeyPoints as input and outputs precise location of the person’s head, in 3D space. Given that the application is designed solely for my personal use, I lock the z view to a fixed height of 190 cm (my height).</p> <p>I had to create a physical grid - on the floor - composed of stickers that are first 1 meters apart (and at a later stage, 50 cms apart), where the grid’s point of origin is the same as the room’s origin. I used good old tape for this practice.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/floor_markers-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/floor_markers-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/floor_markers-1400.webp"></source> <img src="/assets/img/3ddisplay/floor_markers.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="I put scotch tape markers 1 meter apart in my living room." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Markers on the floor of the living room, 1 and 0.5 meter apart. These markers let me know where to stand when capturing training data. </div> <p>The collection of training data was pretty simple: I would stand on each of these markers, and visit a webpage published by FastAPI which asked me to specify which marker I was standing on (by means of filling in two text boxes in a webpage, X and Y distance from the walls respectively). I would enter these values, press a Button, and wait for around 1 minute, during which the camera would take ~50 images of me, calculate features, and store them (in addition to the ground truth labels which I entered in the web page) in a CSV file. I now had the data that I will use to build my custom neural network.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/features_table-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/features_table-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/features_table-1400.webp"></source> <img src="/assets/img/3ddisplay/features_table.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Raw collected data" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The raw data: The first 28 columns point to the X, Y location of each individual Body Pose KeyPoint (nose and left eye only shown). The last 2 columns show ground truths. </div> <p>I took a total of 5500 images, wearing different clothes, moving a little bit while listening to music, and at different times of the day - in an attempt to diversify the data as much as possible. By incorporating these variations, the dataset encompasses a realistic representation of different scenarios and conditions, enabling the network to generalize and adapt to varying circumstances.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/RoomLayout-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/RoomLayout-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/RoomLayout-1400.webp"></source> <img src="/assets/img/3ddisplay/RoomLayout.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Origin Point in the room" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> I set the room corner as the origin point - in retrospect, I should have used the Webcam as the origin point to make my code more reusable in different environments. This diagram shows a sample spot in the room and how that location is interpreted. </div> <p>Data collection is an iterative process - and the data that I had collected was not properly distributed, as seen in the heatmap below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/data_distribution-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/data_distribution-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/data_distribution-1400.webp"></source> <img src="/assets/img/3ddisplay/data_distribution.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Data is not acquired uniformly, leading to bias." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Rows and Columns represent distance from the room's origin point. As you can see, some areas suffer from lack of data (for example, there's a lot of missing data 7.5 meters away from the X wall.) </div> <h4 id="calculating-features">Calculating features</h4> <p>Processing the raw data to extract features can help the Neural network converge faster. To extract meaningful features for my network, I extracted the 18 Keypoints generated by PoseNet. Focusing on symmetric keypoints, which have corresponding “left” and “right” components (such as left_eye, right_eye, left_shoulder, right_shoulder, etc.), I computed three crucial metrics for each:</p> <ol> <li>The pixel distance between the left and right keypoints, providing insights into the relative spatial positioning.</li> <li>the Center: I determined the center location of each symmetric keypoint, offering a reference point for further analysis.</li> <li>Tilt: Lastly, I derived the angular tilt of each symmetric keypoint, enabling a deeper understanding of the body pose and its orientation. By incorporating these calculated features, my network gains a comprehensive understanding of the viewer’s body positioning, enhancing the accuracy and realism of the 3D display experience. Additionally, it “learns” that being at a specific distance to the TV does not necessarily mean ‘looking at the TV’.</li> </ol> <h4 id="building-a-custom-neural-network-to-map-2d-webcam-view-to-3d-location-of-my-head-in-the-room">Building a custom Neural Network to map 2D (webcam view) to 3D (location of my head, in the room)</h4> <p>Determining the architecture of the networks was an interesting exercise, and felt a bit arbitrary. The network is a simple MLP built on Pytorch and trained on my laptop. Omitting batch size, the network takes in 30 features per frame, and maps those to two ground truths (X, and Y). Z was not required, as it was fixed at 190 cms. All values were scaled linearly to fit a range of [0, 1]. a 10-e3 learning rate is used with ADAM, and the network calculates loss according to a simple MAE calculation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/networkperformance1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/networkperformance1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/networkperformance1-1400.webp"></source> <img src="/assets/img/3ddisplay/networkperformance1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="The performance of the Neural Network. This diagram rendered by matplotlib shows my living room (crude format, TV to the left and sofa in the center). the X marks shows where I stood, and the circles show the inference result of my network." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The custom Neural achieves 0.001 loss. That's 0.1% on a linear scale that ranges [0, 1] in both X and Y axis, scaled down from 8 and 6 meters respectively. This means that the error of 0.1% - once re-scaled - will be 0.8 cm and 0.6 cm respectively. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/networkperformance2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/networkperformance2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/networkperformance2-1400.webp"></source> <img src="/assets/img/3ddisplay/networkperformance2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="The network is obviously overfitting - and that's totally OK in my case (I am the only user of this solution after all) But it's worth revisiting at a later stage." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>I try to counterbalance massive overfitting by creating a <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="external nofollow noopener" target="_blank">simple Generator</a> on my dataset on top of the data. The generator adds up to 1 Pixel of noise for all the features collected. I’ve also experimented with self-attention layers (with Layer Normalization, Dropout, and residual connections, but no attention heads), as well as with MLP architectures. It seems that an MLP architecture was suitable and reaches phenomenal outcomes. It is composed of 4 Linear Layers, 30 -&gt; 2048 -&gt; 1024 -&gt; 64 -&gt; 2, with Dropout interspread, and finish with a Sigmoid function.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/network_perf_individual-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/network_perf_individual-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/network_perf_individual-1400.webp"></source> <img src="/assets/img/3ddisplay/network_perf_individual.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Error at twp rows" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Although generally the network performs well, it fails at specific locations in the room (could it be because of lighting, high angle, or something else?). The areas that were guessed incorrectly by the network (marked with a long connecting line) will require more training data. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/features-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/features-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/features-1400.webp"></source> <img src="/assets/img/3ddisplay/features.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Sample features calculated from Raw data." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> For each Body Pose KeyPoints (eyes, shoulders, knees, etc..), some features are calculated. First, a binary [0, 1] determines if this feature is found in the frame (in retrospect, this was feature was completely useless since the parameter is already learnt with the true value of the feature). For each feature detected, if the feature belongs to a pair (left+right), then define the center location, the distance between both points, and the angle of rotation between both points. </div> <h4 id="finding-important-features---pca">Finding important features - PCA</h4> <p>WIP.</p> <h4 id="search-a-hyperparameter-space-for-the-best-settings">Search a Hyperparameter space for the best settings</h4> <p>WIP.</p> <h4 id="creating-a-decoupled-streaming-architecture-fixing-the-latency-problem">Creating a decoupled streaming architecture (fixing the latency problem)</h4> <p>the application components are very synchronous: Each time a client requests an inference, A whole waterfall process has to run (capture image, extract keypoints, extract features, estimate XYZ location in room) - and this process is CPU bound. When the unity client requests the REST API once per Frame, the code on the Nano quickly crashes after few seconds. To resolve this problem, I’ve decided to decouple request for inference, from inferring the data, by creating a memory buffer, and leveraging <a href="https://fastapi.tiangolo.com/tutorial/background-tasks/" rel="external nofollow noopener" target="_blank">FastAPI’s <code class="language-plaintext highlighter-rouge">Background_tasks</code></a>. In the future, I might explore the role of other libraries, such as <a href="https://docs.celeryq.dev/en/stable/getting-started/introduction.html" rel="external nofollow noopener" target="_blank">celery</a>.</p> <ol> <li>A ConfigStore is a dictionary object that includes a time frequency (in my case, max 20 API calls per second). The ConfigStore has a function <code class="language-plaintext highlighter-rouge">is_it_time_to_process_a_new_photo()</code> function, that returns False if less than 1/20 seconds have passed, and true otherwise.</li> </ol> <p>The architecture of the application now looks as follows:</p> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1686284942000" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:1050px;" viewbox="-50 -10 1050 406"><style>#mermaid-1686284942000 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1686284942000 .node circle,#mermaid-1686284942000 .node ellipse,#mermaid-1686284942000 .node polygon,#mermaid-1686284942000 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1686284942000 .node.clickable{cursor:pointer}#mermaid-1686284942000 .arrowheadPath{fill:#333}#mermaid-1686284942000 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1686284942000 .edgeLabel{background-color:#e8e8e8}#mermaid-1686284942000 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1686284942000 .cluster text{fill:#333}#mermaid-1686284942000 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1686284942000 .actor{stroke:#ccf;fill:#ececff}#mermaid-1686284942000 text.actor{fill:#000;stroke:none}#mermaid-1686284942000 .actor-line{stroke:grey}#mermaid-1686284942000 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1686284942000 .messageLine0,#mermaid-1686284942000 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1686284942000 #arrowhead{fill:#333}#mermaid-1686284942000 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1686284942000 .messageText{fill:#333;stroke:none}#mermaid-1686284942000 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1686284942000 .labelText,#mermaid-1686284942000 .loopText{fill:#000;stroke:none}#mermaid-1686284942000 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1686284942000 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1686284942000 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1686284942000 .section{stroke:none;opacity:.2}#mermaid-1686284942000 .section0{fill:rgba(102,102,255,.49)}#mermaid-1686284942000 .section2{fill:#fff400}#mermaid-1686284942000 .section1,#mermaid-1686284942000 .section3{fill:#fff;opacity:.2}#mermaid-1686284942000 .sectionTitle0,#mermaid-1686284942000 .sectionTitle1,#mermaid-1686284942000 .sectionTitle2,#mermaid-1686284942000 .sectionTitle3{fill:#333}#mermaid-1686284942000 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1686284942000 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1686284942000 .grid path{stroke-width:0}#mermaid-1686284942000 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1686284942000 .task{stroke-width:2}#mermaid-1686284942000 .taskText{text-anchor:middle;font-size:11px}#mermaid-1686284942000 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1686284942000 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1686284942000 .taskText0,#mermaid-1686284942000 .taskText1,#mermaid-1686284942000 .taskText2,#mermaid-1686284942000 .taskText3{fill:#fff}#mermaid-1686284942000 .task0,#mermaid-1686284942000 .task1,#mermaid-1686284942000 .task2,#mermaid-1686284942000 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1686284942000 .taskTextOutside0,#mermaid-1686284942000 .taskTextOutside1,#mermaid-1686284942000 .taskTextOutside2,#mermaid-1686284942000 .taskTextOutside3{fill:#000}#mermaid-1686284942000 .active0,#mermaid-1686284942000 .active1,#mermaid-1686284942000 .active2,#mermaid-1686284942000 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1686284942000 .activeText0,#mermaid-1686284942000 .activeText1,#mermaid-1686284942000 .activeText2,#mermaid-1686284942000 .activeText3{fill:#000!important}#mermaid-1686284942000 .done0,#mermaid-1686284942000 .done1,#mermaid-1686284942000 .done2,#mermaid-1686284942000 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1686284942000 .doneText0,#mermaid-1686284942000 .doneText1,#mermaid-1686284942000 .doneText2,#mermaid-1686284942000 .doneText3{fill:#000!important}#mermaid-1686284942000 .crit0,#mermaid-1686284942000 .crit1,#mermaid-1686284942000 .crit2,#mermaid-1686284942000 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1686284942000 .activeCrit0,#mermaid-1686284942000 .activeCrit1,#mermaid-1686284942000 .activeCrit2,#mermaid-1686284942000 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1686284942000 .doneCrit0,#mermaid-1686284942000 .doneCrit1,#mermaid-1686284942000 .doneCrit2,#mermaid-1686284942000 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1686284942000 .activeCritText0,#mermaid-1686284942000 .activeCritText1,#mermaid-1686284942000 .activeCritText2,#mermaid-1686284942000 .activeCritText3,#mermaid-1686284942000 .doneCritText0,#mermaid-1686284942000 .doneCritText1,#mermaid-1686284942000 .doneCritText2,#mermaid-1686284942000 .doneCritText3{fill:#000!important}#mermaid-1686284942000 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1686284942000 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1686284942000 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1686284942000 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1686284942000 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1686284942000 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1686284942000 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1686284942000 #compositionEnd,#mermaid-1686284942000 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284942000 #aggregationEnd,#mermaid-1686284942000 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1686284942000 #dependencyEnd,#mermaid-1686284942000 #dependencyStart,#mermaid-1686284942000 #extensionEnd,#mermaid-1686284942000 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1686284942000 .branch-label,#mermaid-1686284942000 .commit-id,#mermaid-1686284942000 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style> <style>#mermaid-1686284942000{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style> <g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="395" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">PoseNet_Webcam_etc</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="395" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">MemoryObject</tspan></text></g><g><line id="actor2" x1="475" y1="5" x2="475" y2="395" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="400" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">FastAPI_Background_task</tspan></text></g><g><line id="actor3" x1="675" y1="5" x2="675" y2="395" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="600" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">FastAPI</tspan></text></g><g><line id="actor4" x1="875" y1="5" x2="875" y2="395" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="800" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">Unity</tspan></text></g><defs><marker id="arrowhead" refx="5" refy="2" markerwidth="6" markerheight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerwidth="15" markerheight="8" orient="auto" refx="16" refy="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="775" y="93" class="messageText" style="text-anchor: middle;">(GET /) - Hi, I'd like to know the position of the person's head, in 3D</text><line x1="875" y1="100" x2="675" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="575" y="128" class="messageText" style="text-anchor: middle;">Submit a new job to take image/process keyoints/and infer</text><line x1="675" y1="135" x2="475" y2="135" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g></g><g><text x="272.5" y="163" class="messageText" style="text-anchor: middle;">Process one image (waterfall process)</text><line x1="470" y1="170" x2="75" y2="170" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="70" y="172" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="177.5" y="198" class="messageText" style="text-anchor: middle;">Update inference with latest human position</text><line x1="80" y1="205" x2="275" y2="205" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="475" y="233" class="messageText" style="text-anchor: middle;">I'd like to know the position of the person's head, in 3D</text><line x1="675" y1="240" x2="275" y2="240" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="270" y="242" fill="#f4f4f4" stroke="#666" width="10" height="33" rx="0" ry="0"></rect></g><g><text x="477.5" y="268" class="messageText" style="text-anchor: middle;">The person is located here</text><line x1="280" y1="275" x2="675" y2="275" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="775" y="303" class="messageText" style="text-anchor: middle;">You are located here.</text><line x1="675" y1="310" x2="875" y2="310" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><rect x="0" y="330" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="362.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">PoseNet_Webcam_etc</tspan></text></g><g><rect x="200" y="330" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="362.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">MemoryObject</tspan></text></g><g><rect x="400" y="330" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="475" y="362.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="475" dy="0">FastAPI_Background_task</tspan></text></g><g><rect x="600" y="330" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="675" y="362.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="675" dy="0">FastAPI</tspan></text></g><g><rect x="800" y="330" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="875" y="362.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="875" dy="0">Unity</tspan></text></g></svg> </div> <h4 id="building-a-kalman-filter-with-copilot">Building a Kalman Filter with Copilot</h4> <p>A problem that started appearing was jittering. The Neural Networks in play are all pixel-accurate, but based their estimates on a single image without necessarily taking into consideration path of motion. Sampling from this inference point at every Frame Refresh results in jittery path and therefore had to be smoothed out. This was an opportunity for me to implement a Kalman Filter, and to use Github copilot Chat for the first time. My mind was blown at how easy it was to ask for a feature correction, or addition, and the bot would yield code AND explanation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3ddisplay/copilot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3ddisplay/copilot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3ddisplay/copilot-1400.webp"></source> <img src="/assets/img/3ddisplay/copilot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="GitHub copilot at work." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Chatting with my code, and learning how to build a Kalman Filter. </div> <h1 id="wrap-up">Wrap up</h1> <p>Now, as I sit in awe of my living room, I am thrilled to share the fruits of my labor. What was once a conventional flat screen has been transformed into a breathtaking 3D display, capable of projecting immersive visuals that seem to leap out of the screen and into my living space. Whether I’m watching a thrilling action movie, exploring virtual worlds, or simply enjoying the beauty of nature documentaries, the newfound depth and realism of the visuals have redefined my home entertainment experience.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Eyas Taifour. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PNPQFZFHEJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PNPQFZFHEJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>